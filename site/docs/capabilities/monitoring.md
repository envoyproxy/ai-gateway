---
title: Monitoring with Prometheus
sidebar_position: 6
---

# Monitoring with Prometheus

AI Gateway provides built-in Prometheus metrics to help you monitor and observe the performance and usage of your AI models and backends. This guide explains the available metrics and how to set up Prometheus to collect and visualize them.

## Available Metrics

AI Gateway exposes the following Prometheus metrics:

### Request Metrics

| Metric Name | Type | Description | Labels |
|-------------|------|-------------|--------|
| `aigateway_total_latency_seconds` | Histogram | Total time spent processing a request from start to finish | `backend`, `model`, `status` |
| `aigateway_requests_total` | Counter | Total number of requests processed | `backend`, `model`, `status` |

### Token Metrics

| Metric Name | Type | Description | Labels |
|-------------|------|-------------|--------|
| `aigateway_model_tokens_total` | Counter | Total number of tokens processed | `backend`, `model`, `type` (prompt, completion, total) |


## Metric Details

### Total Latency

The `aigateway_total_latency_seconds` metric measures the total time spent processing a request, from the start of receiving the request headers in the external processor to the end of processing the response body. This histogram metric allows you to analyze latency distributions and calculate percentiles (p50, p95, p99) to understand the overall performance of your AI Gateway and the connected backends.

The metric includes the following labels:
- `backend`: The backend service handling the request (e.g., "openai", "awsbedrock")
- `model`: The specific model used (e.g., "gpt-4o", "llama3")
- `status`: The request status ("success" or "error")

### Tokens Total

The `aigateway_model_tokens_total` counter metric tracks the number of tokens processed, broken down by:
- `prompt`: Input tokens sent to the model
- `completion`: Output tokens generated by the model
- `total`: Total tokens (prompt + completion)

The labels provide granular visibility:
- `backend`: The backend service processing the tokens
- `model`: The specific model used
- `type`: Token type ("prompt", "completion", or "total")


## Enabling Prometheus Metrics

AI Gateway exposes Prometheus metrics by default on port 9190. You can configure the metrics endpoint using the `--metricsAddr` flag when starting the extproc server:

```bash
--metricsAddr=":9190"
```

The metrics are available at the `/metrics` endpoint, e.g., `http://localhost:9190/metrics`.

## Setting Up Prometheus

To collect these metrics with Prometheus, add the following job to your Prometheus configuration:

```yaml
scrape_configs:
  - job_name: 'ai-gateway'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:9190']
```

## Grafana Dashboard

AI Gateway provides a pre-configured Grafana dashboard to visualize these metrics. You can import the dashboard from the `configs/grafana-dashboard.json` file in the AI Gateway repository.

The dashboard includes visualizations for:
- Request rates and success percentages
- Latency distributions by model and backend
- Token usage patterns
- First token and inter-token latency trends

## Example Prometheus Queries

Here are some useful Prometheus queries for monitoring your AI Gateway:

### Request Success Rate

```
sum(rate(aigateway_requests_total{status="success"}[5m])) by (backend, model) / 
sum(rate(aigateway_requests_total[5m])) by (backend, model)
```

This query calculates the success rate for each backend and model combination over a 5-minute window.

### 95th Percentile Latency by Model

```
histogram_quantile(0.95, sum(rate(aigateway_total_latency_seconds_bucket{status="success"}[5m])) by (le, model))
```

This query shows the 95th percentile latency for successful requests by model, helping you identify which models might be experiencing performance issues.

### Token Usage by Model

```
sum(rate(aigateway_model_tokens_total{type="total"}[1h])) by (model)
```

This query shows the token consumption rate by model over the last hour, useful for cost monitoring and capacity planning.

### First Token Latency (p50)

```
histogram_quantile(0.5, sum(rate(aigateway_first_token_latency_seconds_bucket[5m])) by (le, model))
```

This query shows the median (p50) time to first token by model, a key indicator of perceived responsiveness.

