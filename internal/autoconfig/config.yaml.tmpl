# Copyright Envoy AI Gateway Authors
# SPDX-License-Identifier: Apache-2.0
# The full text of the Apache license is available in the LICENSE file at
# the root of the repo.
{{- if .MCPBackendRefs }}

# Configuration for Envoy AI Gateway with MCP servers
{{- else if .OpenAI }}

# Configuration for Envoy AI Gateway with OpenAI compatible endpoint
{{- end }}
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: aigw-run
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: aigw-run
  namespace: default
spec:
  gatewayClassName: aigw-run
  listeners:
    - name: http
      protocol: HTTP
      port: 1975
  infrastructure:
    parametersRef:
      group: gateway.envoyproxy.io
      kind: EnvoyProxy
      name: envoy-ai-gateway
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: EnvoyProxy
metadata:
  name: envoy-ai-gateway
  namespace: default
spec:
  logging:
    level:
{{- if .Debug }}
      default: debug
{{- else }}
      default: error
{{- end }}
  telemetry:
    accessLog:
      settings:
        - sinks:
            - type: File
              file:
                path: /dev/stdout
          format:
            type: JSON
            json:
{{- if .OpenAI }}
              # LLM specific fields. The properties in the dynamic metadata
              # expressions must match the ones defined in the AIGatewayRoute
              # llmRequestCosts field.
              genai_model_name: "%REQ(X-AI-EG-MODEL)%"
              genai_model_name_override: "%DYNAMIC_METADATA(io.envoy.ai_gateway:model_name_override)%"
              genai_backend_name: "%DYNAMIC_METADATA(io.envoy.ai_gateway:backend_name)%"
              genai_tokens_input: "%DYNAMIC_METADATA(io.envoy.ai_gateway:llm_input_token)%"
              genai_tokens_output: "%DYNAMIC_METADATA(io.envoy.ai_gateway:llm_output_token)%"
{{- end }}
              # A few default fields
              start_time: "%START_TIME%"
              method: "%REQ(:METHOD)%"
              x-envoy-origin-path: "%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%"
              response_code: "%RESPONSE_CODE%"
              connection_termination_details: "%CONNECTION_TERMINATION_DETAILS%"
              upstream_transport_failure_reason: "%UPSTREAM_TRANSPORT_FAILURE_REASON%"
              bytes_received: "%BYTES_RECEIVED%"
              bytes_sent: "%BYTES_SENT%"
              duration: "%DURATION%"
              user-agent: "%REQ(USER-AGENT)%"
              x-request-id: "%REQ(X-REQUEST-ID)%"
              upstream_host: "%UPSTREAM_HOST%"
              upstream_cluster: "%UPSTREAM_CLUSTER%"
              upstream_local_address: "%UPSTREAM_LOCAL_ADDRESS%"
              downstream_local_address: "%DOWNSTREAM_LOCAL_ADDRESS%"
              downstream_remote_address: "%DOWNSTREAM_REMOTE_ADDRESS%"
---
{{- if .OpenAI }}
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIGatewayRoute
metadata:
  name: aigw-run
  namespace: default
spec:
  parentRefs:
    - name: aigw-run
      kind: Gateway
      group: gateway.networking.k8s.io
  # Simple rule: route everything to OpenAI backend
  rules:
    - matches:
        - headers:
            - type: RegularExpression
              name: x-ai-eg-model
              value: .*
      backendRefs:
        - name: {{ .OpenAI.BackendName }}
          namespace: default
      timeouts:
        request: 120s
  # Configure the LLM request costs so they can be included in the Envoy access logs
  llmRequestCosts:
    - metadataKey: llm_input_token
      type: InputToken
    - metadataKey: llm_output_token
      type: OutputToken
---
{{- end }}
{{- if .MCPBackendRefs }}
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: MCPRoute
metadata:
  name: mcp-route
  namespace: default
spec:
  parentRefs:
    - name: aigw-run
      kind: Gateway
      group: gateway.networking.k8s.io
  path: "/mcp"
  backendRefs:
  {{- range .MCPBackendRefs }}
    - name: {{ .BackendName }}
      kind: Backend
      group: gateway.envoyproxy.io
      path: "{{ .Path }}"
      {{- if .IncludeTools }}
      toolSelector:
        include:
          {{- range .IncludeTools }}
          - {{ . }}
          {{- end }}
      {{- end }}
      {{- if .APIKey }}
      securityPolicy:
        apiKey:
          secretRef:
            name: {{ .BackendName }}-token
      {{- end }}
  {{- end }}
---
{{- end }}
{{- range .Backends }}
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: {{ .Name }}
  namespace: default
spec:
  endpoints:
    - fqdn:
        hostname: {{ .Hostname }}
        port: {{ .Port }}
---
{{- end }}
{{- if .OpenAI }}
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: {{ .OpenAI.BackendName }}
  namespace: default
spec:
  timeouts:
    request: 3m
  schema:
    name: {{ .OpenAI.SchemaName }}
{{- if .OpenAI.Version }}
    version: "{{ .OpenAI.Version }}"
{{- end }}
  backendRef:
    name: {{ .OpenAI.BackendName }}
    kind: Backend
    group: gateway.envoyproxy.io
    namespace: default
{{- if or .OpenAI.OrganizationID .OpenAI.ProjectID }}
  headerMutation:
    set:
{{- if .OpenAI.OrganizationID }}
      - name: "OpenAI-Organization"
        value: "{{ .OpenAI.OrganizationID }}"
{{- end }}
{{- if .OpenAI.ProjectID }}
      - name: "OpenAI-Project"
        value: "{{ .OpenAI.ProjectID }}"
{{- end }}
{{- end }}
---
{{- end }}
{{- range .Backends }}
{{- if .NeedsTLS }}
apiVersion: gateway.networking.k8s.io/v1alpha3
kind: BackendTLSPolicy
metadata:
  name: {{ .Name }}-tls
  namespace: default
spec:
  targetRefs:
    - group: 'gateway.envoyproxy.io'
      kind: Backend
      name: {{ .Name }}
  validation:
    wellKnownCACertificates: "System"
    hostname: {{ .OriginalHostname }}
---
{{- end }}
{{- end }}
# By default, Envoy Gateway sets the buffer limit to 32kiB which is not
# sufficient for AI workloads. This ClientTrafficPolicy sets the buffer limit
# to 50MiB as an example.
# TODO: Remove after https://github.com/envoyproxy/ai-gateway/issues/1212
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: ClientTrafficPolicy
metadata:
  name: client-buffer-limit
  namespace: default
spec:
  targetRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      name: aigw-run
  connection:
    bufferLimit: 50Mi
---
{{- if .OpenAI }}
apiVersion: v1
kind: Secret
metadata:
  name: openai-apikey
  namespace: default
type: Opaque
stringData:
{{- if eq .OpenAI.SchemaName "AzureOpenAI" }}
  apiKey: ${AZURE_OPENAI_API_KEY}
{{- else }}
  apiKey: ${OPENAI_API_KEY}
{{- end }}
---
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: BackendSecurityPolicy
metadata:
  name: openai-apikey
  namespace: default
spec:
  targetRefs:
    - group: aigateway.envoyproxy.io
      kind: AIServiceBackend
      name: {{ .OpenAI.BackendName }}
{{- if eq .OpenAI.SchemaName "AzureOpenAI" }}
  type: AzureAPIKey
  azureAPIKey:
    secretRef:
      name: openai-apikey
{{- else }}
  type: APIKey
  apiKey:
    secretRef:
      name: openai-apikey
{{- end }}
{{- end }}
{{- range .MCPBackendRefs }}
{{- if .APIKey }}
kind: Secret
apiVersion: v1
metadata:
  name: {{ .BackendName }}-token
  namespace: default
type: Opaque
stringData:
  apiKey: {{ .APIKey }}
---
{{- end }}
{{- end }}
